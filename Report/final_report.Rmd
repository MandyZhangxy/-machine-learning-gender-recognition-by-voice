---
title: "Gender Recognition by Voice"
author: "Xinyu Zhang, Xiaochi Ge, Wenye Ouyang"
date: "12/4/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,comment=NA)
knitr::opts_chunk$set(fig.width=5, fig.height=3.5,fig.align='center') 
```

# I. Introduction
Our SMART question is: How to use classification models to recognize gender by their voice? 

The voice of a person contains many different properties, and these properties often reflect the certain information about the person, such as gender. The objective of this research is to build a gender recognition system based on different classification algorithms. The dataset includes the measurement of each voice sample's auditory features which are based upon acoustic properties of the voice. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages. 


```{r echo = FALSE, warning=FALSE, message=FALSE}
#install.packages("caTools")
#install.packages("caret")
#install.packages("e1071")
#install.packages("pROC")
#install.packages("randomForest", dependencies = FALSE)
library(randomForest)
library(ggplot2)
library(caret)
library(pROC)
library(caTools)
library(class)
library(tidyr)
library(dplyr)
library(stats)
```

# II. Data information & Exploratory Data Analysis

## 1. Data Information
The dataset consists of acoustic properties, such as frequencies, of audio samples generated by human voices. The focus of the project to use these properties to accurately predict gender based on voice. 

The source of the data is from this website: [**Primary Objects**](http://www.primaryobjects.com/2016/06/22/identifying-the-gender-of-a-voice-using-machine-learning/) by *KORY BECKER*. In this dataset, it contains 3169 voice samples and 21 columns (our target variable is `label`). 

- duration: length of signal
- meanfreq: mean frequency (in kHz)
- sd: standard deviation of frequency
-	median: median frequency (in kHz)
-	Q25: first quantile (in kHz)
-	Q75: third quantile (in kHz)
-	IQR: interquantile range (in kHz)
-	skew: skewness (see note in specprop description)
-	kurt: kurtosis (see note in specprop description)
-	sp.ent: spectral entropy
-	sfm: spectral flatness
-	mode: mode frequency
-	centroid: frequency centroid (see specprop)
-	peakf: peak frequency (frequency with highest energy)
-	meanfun: average of fundamental frequency measured across acoustic signal
-	minfun: minimum fundamental frequency measured across acoustic signal
-	maxfun: maximum fundamental frequency measured across acoustic signal
-	meandom: average of dominant frequency measured across acoustic signal
-	mindom: minimum of dominant frequency measured across acoustic signal
-	maxdom: maximum of dominant frequency measured across acoustic signal
-	dfrange: range of dominant frequency measured across acoustic signal
-	modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range
- **label**: female and male. (This is **target** variable)


Three classification models were considered for gender recognition:

1. Random Forest
2. K-Nearest Neighbour 
3. Logistic Regression

Finally, the confusion matricies and highest accuracies will be used to determine the best classification model. 


## 2. Exploratory Data Analysis

```{r echo=FALSE, warning=FALSE}
# Downloading the data
setwd("/Users/MandyZhang/Desktop/DATS6101-group-project-2/Report/")
voice = read.csv("../data/voice.csv")
#scaled_voice = read.csv(".../data/scaled_voice.csv")
scaled_voice = voice

#### eda:
gender = voice %>% group_by(label)
gender = gender %>% summarise(n=n())
# bar plot
ggplot(gender, aes(label, n, color = label, fill = label)) + geom_bar(stat = 'identity',alpha=0.3,width=0.5)+
  geom_text(aes(label = n), vjust=-0.3, size = 2.3) +
  labs(x = "Gender", y = "Count", size = 3) + ggtitle("Number of People in Each Gender") + theme_light()

# distribution of feature:
# Convert dataframe to key-value pairs
pair = gather(voice[, 1:20])
#plot distribution of all features
ggplot(pair, aes(value)) + facet_wrap(~ key, scales = "free") + geom_density()+
  labs(x = "feature in each column") + ggtitle("Density of all variables")+
  theme(plot.title = element_text(face = "bold", size = 8, hjust = 0.5),
        axis.title.x = element_text(face = "bold",size = 5, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 5),
        axis.text.x = element_text(vjust = 0.1, hjust = 0.1)) 

# compare two variables:
# modulation index and mean frequency:
ggplot(voice, aes(x=modindx, y=meanfreq, group=label)) + geom_point(aes(colour = label), size = 1,alpha=0.55) +
  labs(x = "Modulation Index", y = "Mean Frequency") + ggtitle("Modulation Index vs. Mean Frequency")+
  theme(plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
        axis.title.x = element_text(face = "bold", size = 15, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 15),
        axis.text.x = element_text(vjust = 0.1, hjust = 0.1)) + theme_light() +  scale_color_hue(l=38, c=65)

# Skew and Kurtosis
ggplot(voice, aes(skew, kurt, group=label)) + geom_line(aes(colour = label), size = 0.7,alpha=0.9) +
  labs(x = "Skew", y = "Kurtosis", size = 3) + ggtitle("Skew vs. Kurtosis")+
  theme(plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
        axis.title.x = element_text(face = "bold",size = 15, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 15),
        axis.text.x = element_text(vjust = 0.1, hjust = 0.1)) + theme_light() + scale_color_hue(l=38, c=65)

# SD vs. Mean Frequency
ggplot(voice, aes(x=sd, y=meanfreq, group=label)) + geom_point(aes(colour = label), size = 1,alpha=0.55) +
  labs(x = "Standard Deviation of Frequency", y = "Mean Frequency") + ggtitle("Standard Deviation vs. Mean Frequency")+
  theme(plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
        axis.title.x = element_text(face = "bold", size = 15, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 15),
        axis.text.x = element_text(vjust = 0.1, hjust = 0.1)) + theme_light() +  scale_color_hue(l=38, c=65)

#Avg of Fundmental Frequency vs Mean Frequency
ggplot(voice, aes(x=meanfun, y=meanfreq, group=label)) + geom_point(aes(colour = label), size = 1,alpha=0.55) +
  labs(x = "Average of Fundmental Frequency", y = "Mean Frequency") + ggtitle("Average of Fundmental Frequency vs. Mean Frequency")+
  theme(plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
        axis.title.x = element_text(face = "bold", size = 15, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 15),
        axis.text.x = element_text(vjust = 0.1, hjust = 0.1)) + theme_light() +  scale_color_hue(l=38, c=65)

#SD vs Avg of Fundamental Frenquency
ggplot(voice, aes(x=meanfun, y=sd, group=label)) + geom_point(aes(colour = label), size = 1,alpha=0.55) +
  labs(x = "Average of Fundmental Frequency", y = "Standard Deviation") + ggtitle("Average of Fundmental Frequency vs. Standard Deviation")+
  theme(plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
        axis.title.x = element_text(face = "bold", size = 15, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 15),
        axis.text.x = element_text(vjust = 0.1, hjust = 0.1)) + theme_light() +  scale_color_hue(l=38, c=65)

#Q25 vs IQR
ggplot(voice, aes(x=Q25, y=IQR, group=label)) + geom_point(aes(colour = label), size = 1,alpha=0.55) +
  labs(x = "First Quantile", y = "Interquantile") + ggtitle("First Quantile vs. Interquantile")+
  theme(plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
        axis.title.x = element_text(face = "bold", size = 15, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 15),
        axis.text.x = element_text(vjust = 0.1, hjust = 0.1)) + theme_light() +  scale_color_hue(l=38, c=65)

#spectral entropy vs spectral flatness
ggplot(voice, aes(x=sp.ent, y=sfm, group=label)) + geom_point(aes(colour = label), size = 1,alpha=0.55) +
  labs(x = "Spectral Entropy", y = "Spectral Flatness") + ggtitle("Spectral Entropy vs. Spectral Flatness")+
  theme(plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
        axis.title.x = element_text(face = "bold", size = 15, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 15),
        axis.text.x = element_text(vjust = 0.1, hjust = 0.1)) + theme_light() +  scale_color_hue(l=38, c=65)
```

# III. Data Preprocessing
## 1.Set training and testing

- Randomly select 70% train and 30% test groups
- After feature selections, we will only include selected features in training and testing.
```{r echo=FALSE, warning=FALSE}
# randomly select 70% train and 30% test groups:
set.seed(1)
#ind = sample.split(Y = scaled_voice["label"], SplitRatio = 0.7)
#train = scaled_voice[ind, ]
#test = scaled_voice[-ind, ]

smp_size = floor(0.7 * nrow(scaled_voice))
train_ind = sample(seq_len(nrow(scaled_voice)), size = smp_size)

train = scaled_voice[train_ind, ]
test = scaled_voice[-train_ind, ]


```

## 2.Feature selection
### Using Random Forest Importance to select features. 

Forest error rate depends on two things:

1. The correlation between any two trees in the forest. Increasing the correlation increases the forest error rate.
2. The strength of each individual tree in the forest. 

A tree with a low error rate is a strong classifier. Therefore, we want to increase the strength of the individual trees and decrease decreases the forest error rate.

However, reducing m reduces both the correlation and the strength. Increasing it increases both. Somewhere in between is an "optimal" range of mtry - usually quite wide. Using the oob error rate (see  the plot below) can give a value of m in the range can quickly be found. 

Therefore, for feature selection, we need to do two steps:

1. Find the best mtry (number of variables selected at each split)
2. According to plot of importance in the desending order, we select top 7 important features.


```{r echo = FALSE, warning=FALSE}
# First, use random forest model as classification
# Second, Fitting the model:
# to find the best mtry:
train = data.frame(train)
test= data.frame(test)
bestmry = tuneRF(x = train[, -21], train$label, 
                 ntreeTry = 300, stepFactor = 1.5, improve = 0.001,
                 trace = TRUE, plot = TRUE, importance = TRUE)

bestmry = data.frame(bestmry)
bestmry = bestmry$mtry[which.min(bestmry$OOBError)]
print(paste("Therefore, based on the plot above, the best number of variables at each split is", bestmry))

trainingmodel = randomForest(label~., data = train, mtry = bestmry, ntree = 350)
importance = data.frame(importance(trainingmodel))
variables = rownames(importance)
importance$variables = variables
# plot 1
varImpPlot(trainingmodel)

```

```{r eacho = FALSE, warning=FALSE }
# plot 2 importances of feautres
ggplot(data = importance, aes(
  x = reorder(variables, -MeanDecreaseGini),
  y = MeanDecreaseGini)) + geom_bar(stat = 'identity',fill = "#FF6666",alpha=0.75) + 
  theme_light() + xlab("Variabels")+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_text(aes(label = round(MeanDecreaseGini)), vjust=-0.3, size = 2.5)+
  ggtitle("\n\nImportance of Variables in descending order") + 
  theme(plot.title = element_text(face = "bold", size = 10, hjust = 0.5),
        axis.title.x = element_text(face = "bold",size = 5, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 5),
        axis.text.x=element_text(angle = -75, hjust = 0))

# trim the train and test data frame with only selected variabels:
selected = importance$variables[order(importance$MeanDecreaseGini, decreasing = T)[1:7]]
selected = append(selected, "label")
train = train[ , names(train) %in% selected]
test = test[ , names(test) %in% selected]

print("The selected features and the target variable are:")
print(names(train))
```


# IV. Models Building

## 1. Random Forest Classification

- Set parameters for random forest model
- Plot the ROC/AUC and confusion matrix

```{r echo=FALSE, warning=FALSE}
# retraining model with selected variabels according to importance:
trainingmodel = randomForest(label~., data = train, mtry = bestmry, ntree = 350)  
predictionWithClass = predict(trainingmodel, test, type = "class")
t = table(predictions = predictionWithClass, actual = test$label)
t

# Accuracy metric/rate
print("In the Random Forest model, the accuracy rate is:")
paste(round(sum(diag(t))/sum(t)*100,2),"%")

# plot roc/auc:
predictionWithProbs = predict(trainingmodel, test, type = "prob")
plot(roc(test$label, predictionWithProbs[ ,2]), main = "Random Forest AUC/ROC")

```

```{r echo=FALSE, warning=FALSE}
# ploting confusion matrix in ggplot:
CM = confusionMatrix(predictionWithClass, test$label)
print(CM)
cm = as.data.frame(CM$table)
ggplot(data = cm, mapping = aes(x = Reference, y = Prediction)) + 
  geom_tile(aes(fill = Freq), color = "white") + 
  scale_fill_gradient(low = "white", high = "#ffb5e9")  + 
  geom_text(aes(label = Freq),size = 7) + theme(legend.position = "none") + 
  ggtitle(paste("Confusion Matrix with Accuracy rate ", round(100*CM$overall[1],2), "%"))+
  theme(plot.title = element_text(face = "bold", size = 10, hjust = 0.5),
        axis.title.x = element_text(face = "bold",size = 8, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 8))
```

Based on the confusion matrix, there are 468 samples are predicted correctly as females; however, there are 10 samples predicted as males but are actually females. There are 465 samples that are predicted correctly as males; however, there are 8 samples that are predicted as female but are in fact male.

Based on the elbow method, we find that the first segment of line in AUC is almost parallel to the y-axis and the angel of the left corner is nearly 90 degree, which means that the area under curve is close 100, and also means that this is a good model. 

The accuracy rate is 98.11% and it is in the 95% confidence interval with the p-value far smaller than 0.05. Therefore, it can be concluded that this accuracy is statistically significant. 

## 2. K-Nearest Neighbour classification

- Set parameter k = 7. When k=7, compared to other k values, the accuracy reach the highest. 
- Plot ROC/AUC and confusion matrix

```{r echo = FALSE, warning=FALSE}
knn_pred = knn(train = train[, -8], test = test[ ,-8], cl=train[, "label"], k=7, prob = TRUE)

#plot ROC
plot(roc(test$label,as.numeric(knn_pred)), main = "KNN AUC/ROC")

print("The accuracy rate in KNN is:")
paste(round(sum(knn_pred == test[, 8])/length(test[,8])*100,2),"%")

CM = confusionMatrix(knn_pred, test$label)
print(CM)
cm = as.data.frame(CM$table)
ggplot(data = cm, mapping = aes(x = Reference, y = Prediction)) + 
  geom_tile(aes(fill = Freq), color = "white") + 
  scale_fill_gradient(low = "white", high = "#ffb5e9")  + 
  geom_text(aes(label = Freq),size = 7) + theme(legend.position = "none") + 
  ggtitle(paste("Confusion Matrix with Accuracy rate ", round(100*CM$overall[1],2), "%"))+
  theme(plot.title = element_text(face = "bold", size = 10, hjust = 0.5),
        axis.title.x = element_text(face = "bold",size = 8, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 8))

```


Based on the confusion matrix, there are 461 samples are predicted correctly as females; however, there are 17 samples predicted as males but are actually females. There are 466 samples that are predicted correctly as males; however, there are 7 samples that are predicted as female but are in fact male.

The accuracy is 97.48% and is in the 95% confidence interval with the p-value far smaller than 0.05. Therefore, we can say that this accuracy is statistically significant. 

## 3. Logistic Regression

```{r echo = FALSE, warning=FALSE,fig.align='center'}

numeric = ifelse(train$label == 'male',1,0)
logitmodel<- glm(label~.,family = binomial(link =  "logit"), train,control = list(maxit = 50))
summary(logitmodel)
```

Based on the summary table above, the P-values of `sd`, `Q25` and `centroid` are larger than 0.05, which means those variables are not significant. Therefore, those variables need to be remove in the next model.

```{r}
# based on summary, we select features with high significancy
logitmodel2<- glm(label~IQR+sp.ent+sfm+meanfun,family = binomial(link =  "logit"), train,control = list(maxit = 50))
summary(logitmodel2)
```

From the second summary:

1. all the variables in the table above are very sinificant since the p-values are far smaller than 0.05. 
2. For every one unit increase in `IQR`, log odds of gender as female increases by 55.11; For every one unint increase in `sp.ent`,the log odds of fender as female increase by 47.884. For every one unit increase in `sfm`, the log odds of gender as female decrease -14.038 (more likely to be male). For every one unit increase in `meanfun`, the log odds of gender as female decrease 155.123 (more likely to be male).
3. The Residual deviance increase from 448.89 to 451.62, which means the model is fitting good.
4. The Deviance Residuals is symmetrically distributed at center, a little bit skewed to the right.
5. the AIC dropped from 464.89 to 461.62, which means the this model is fitting good as well.



```{r}
# predictions:
predictionWithClass_lg = predict(logitmodel2, test[-8],  type='response')
predictionWithClass_lg = ifelse(predictionWithClass_lg>0.5,1,0)
t = table(predictions= predictionWithClass_lg, actual = test$label)
t
# Accuracy metric/rate
print("The accuracy rate in Logistic Regression is:")
paste(round(sum(diag(t))/sum(t)*100,2),"%")

# ploting ROC curve and calculating AUC metric
plot(roc(test$label, predictionWithClass_lg), main = "AUC/ROC")


# plot confusion matrix
lg = predictionWithClass_lg
lg = factor(lg, levels = c(0,1), labels = c("female", "male"))
CM = confusionMatrix(lg, test$label)
print(CM)
cm = as.data.frame(CM$table)

ggplot(data = cm, mapping = aes(x = Reference, y = Prediction)) + 
  geom_tile(aes(fill = Freq), color = "white") + 
  scale_fill_gradient(low = "white", high = "#ffb5e9")  + 
  geom_text(aes(label = Freq),size = 7) + theme(legend.position = "none") + 
  ggtitle(paste("Confusion Matrix with Accuracy rate ", round(100*CM$overall[1],2), "%"))+
  theme(plot.title = element_text(face = "bold", size = 10, hjust = 0.5),
        axis.title.x = element_text(face = "bold",size = 8, vjust = 1),
        axis.title.y = element_text(face = "bold", size = 8))

```


Based on the confusion matrix, there are 464 samples that are predicted correctly as females; however, there are 14 samples that are predicted as males but are actually females. There are 464 samples that are predicted correctly as males; however, there are 9 samples are predicted as females but are in fact males.

The accuracy is 97.58% and is in the 95% confidence interval with the p-value far smaller than 0.05. Therefore, this accuracy is statistically significant. 


# V. Conclusion

- The accuracy rates of all three models are over 97%, all the models performed well on prediction. Among these three, the random forest classification is the best, since its accuracy reaches 98.11%. Therefore,  random forest best model for gender recognition.

- The reason why all models have such high accuracy is that the input dataset is perfectly balanced and has no missing values. If our dataset is imbalanced or has many missing values, we need to rebalance the data and also do the imputation, which will decrease the performance of madels. 

- Gender can be recognized by voice. We did three recognitions in demo. For the first trial, the person spoke too short, and there were some speaker' noise during recording, the result was false. The rest of two were correct. 

- After finishing the major parts of the project, we are still curious that what we can do next to make our machine smarter. What if people disguise their voice? What if we add some feigned voices into the dataset? Can this machine still do a good job? 

- For feature selection, there might be some better algorithms to perform dimention reduction such as principal component analysis (PCA) or independent component analysis (ICA). 

- In validation process, we only randomly select test for one time. It's more appropriate if we did 10-fold cross validation. Then, the accuracy rate will be more 'accurate'. 

